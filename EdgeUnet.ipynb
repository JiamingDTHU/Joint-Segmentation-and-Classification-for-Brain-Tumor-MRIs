{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Some useful utilities'''\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def identical(img):\n",
    "    return img\n",
    "\n",
    "def rot90(img):\n",
    "    result=np.rot90(img, 1, axes=(0, 1))\n",
    "    return result.copy()\n",
    "\n",
    "def rot180(img):\n",
    "    result=np.rot90(img, 2, axes=(0, 1))\n",
    "    return result.copy()\n",
    "\n",
    "def rot270(img):\n",
    "    result=np.rot90(img, 3, axes=(0, 1))\n",
    "    return result.copy()\n",
    "\n",
    "def hormir(img):\n",
    "    result=np.fliplr(img)\n",
    "    return result.copy()\n",
    "\n",
    "def vertmir(img):\n",
    "    result=np.flipud(img)\n",
    "    return result.copy()\n",
    "\n",
    "def medianFilter(img, kernelshape=(3, 3), paddle='zero'):\n",
    "    (M, N)=img.shape\n",
    "    (m, n)=kernelshape\n",
    "    result=np.zeros(img.shape, float)\n",
    "    if paddle == 'zero':\n",
    "        temp=np.zeros((M+2*int(m/2), N+2*int(n/2)))\n",
    "    else:\n",
    "        temp=np.zeros((M+2*int(m/2), N+2*int(n/2)))\n",
    "    temp[int(m/2):int(m/2)+M, int(n/2):int(n/2)+N]=img.copy()\n",
    "    for i in range(0, M):\n",
    "        for j in range(0, N):\n",
    "            result[i, j]=np.median(temp[i:i+m, j:j+n].copy())\n",
    "    return result\n",
    "\n",
    "def preprocessing(img):\n",
    "    normed=((img-np.min(img))/(np.max(img)-np.min(img))*255).astype(np.uint8)\n",
    "    gaussed=cv2.GaussianBlur(normed, (5, 5), 0.5)\n",
    "    # plt.imshow(gaussed, 'gray')\n",
    "    # plt.title('gauss')\n",
    "    # plt.show()\n",
    "    normed=((gaussed-np.min(gaussed))/(np.max(gaussed)-np.min(gaussed))*255).astype(np.uint8)\n",
    "    meded=cv2.medianBlur(normed, 5)\n",
    "    # plt.imshow(meded, 'gray')\n",
    "    # plt.title('median')\n",
    "    # plt.show()\n",
    "    normed=((meded-np.min(meded))/(np.max(meded)-np.min(meded))*255).astype(np.uint8)\n",
    "    clahe=cv2.createCLAHE(2., (8, 8))\n",
    "    enhanced=clahe.apply(normed)\n",
    "    # plt.imshow(enhanced, 'gray')\n",
    "    # plt.title('contrast enhance')\n",
    "    # plt.show()\n",
    "    result=(enhanced-np.min(enhanced))/(np.max(enhanced)-np.min(enhanced))\n",
    "    return result.copy()\n",
    "\n",
    "def dataAug(img, mask):\n",
    "    trans_func=np.random.choice([identical, rot90, rot180, rot270, hormir, vertmir])\n",
    "    img_res, mask_res=trans_func(img), trans_func(mask)\n",
    "    return img_res, mask_res\n",
    "\n",
    "def myCrossEntropyLoss(output, target):\n",
    "    output=output.detach().numpy()[0, 1].reshape((512, 512))\n",
    "    target=target.detach().numpy().reshape((512, 512))\n",
    "    loss=-np.sum(target*np.log(output+1e-12)+(1-target)*np.log(1-output+1e-12))/(512**2)\n",
    "    return loss\n",
    "\n",
    "def my_dice_score(set_A, set_B):\n",
    "    inter=np.sum(set_A*set_B)\n",
    "    union=np.sum(set_A+set_B)\n",
    "    return 2*inter/(union+1e-12)\n",
    "\n",
    "def dilation(src, kernel, pad_value=0, mode='b'):\n",
    "    (M, N)=src.shape\n",
    "    (m, n)=kernel.shape\n",
    "    kernel=np.rot90(kernel, k=2, axes=(0, 1))\n",
    "    dst=src.copy()\n",
    "    if ~pad_value:\n",
    "        padded=np.zeros((M+2*m//2, N+2*n//2))\n",
    "    elif pad_value:\n",
    "        padded=np.ones((M+2*m//2, N+2*n//2))\n",
    "    padded[m//2:m//2+M, n//2:n//2+N]=src.copy()\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            window=padded[i:i+m, j:j+n].copy()\n",
    "            # 若与A的交不为空\n",
    "            if (kernel.astype(np.bool8) & window.astype(np.bool8)).any():\n",
    "                dst[i, j]=1\n",
    "            else:\n",
    "                dst[i, j]=0\n",
    "    return dst\n",
    "\n",
    "def getEdge(src):\n",
    "    kernel=np.ones((3, 3))\n",
    "    return (dilation(src, kernel)-src).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import h5py\n",
    "from utils import *\n",
    "\n",
    "class TumorDataset(Dataset):\n",
    "    def __init__(self, dataset_dir: str, train: bool = True, transform: transforms = None):\n",
    "        self.dataset_dir=dataset_dir\n",
    "        self.transform=transform\n",
    "        self.train=train\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        '''90% training set and 10% testing set'''\n",
    "        if self.train:\n",
    "            return 2742\n",
    "        else:\n",
    "            return 307\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''Get data from dataset and return its image, mask and label fields'''\n",
    "        for i in os.walk(self.dataset_dir):\n",
    "            name_list=i[2]\n",
    "            try:\n",
    "                name_list.remove('.DS_Store')\n",
    "            except:\n",
    "                pass\n",
    "        path=self.dataset_dir+name_list[idx]\n",
    "        image=self.load(path, 'image')\n",
    "        mask=self.load(path, 'tumorMask')\n",
    "        edge=getEdge(mask)\n",
    "        label=int(self.load(path, 'label'))-1\n",
    "        image=preprocessing(image) # 高斯模糊，中值滤波， 对比度增强， 归一化\n",
    "        image, mask=dataAug(image, mask) # 数据增强，包括旋转，镜像，对图像与mask施加同样的操作\n",
    "        if self.transform:\n",
    "            image=self.transform(image)\n",
    "        \n",
    "        return (\n",
    "            torch.as_tensor(image).float(), \n",
    "            torch.as_tensor(mask).long(), \n",
    "            torch.as_tensor(label).long(),\n",
    "            torch.as_tensor(edge).float()\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def load(path, field):\n",
    "        '''Load and preprocess a single .mat data file'''\n",
    "        \n",
    "        assert field in ['image', 'label', 'tumorMask', 'tumorBorder'], 'Incorrect data field'\n",
    "        \n",
    "        with h5py.File(path, 'r') as f:\n",
    "            result=np.array(f['cjdata'][field])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge Unet Network definition\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBNReLU(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.layers=torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding), \n",
    "            torch.nn.BatchNorm2d(out_channels), \n",
    "            torch.nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class ImageEncodeBlock1(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.layers=torch.nn.Sequential(\n",
    "            ConvBNReLU(in_channels, out_channels),\n",
    "            ConvBNReLU(out_channels, out_channels),\n",
    "            ConvBNReLU(out_channels, out_channels),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class ImageEncodeBlock2(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.layers=torch.nn.Sequential(\n",
    "            ConvBNReLU(in_channels, out_channels),\n",
    "            ConvBNReLU(out_channels, out_channels),\n",
    "            ConvBNReLU(out_channels, out_channels),\n",
    "            ConvBNReLU(out_channels, out_channels),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class ImageEncodeBlock3(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.layers=torch.nn.Sequential(\n",
    "            ConvBNReLU(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBNReLU(out_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            torch.nn.Dropout2d(0.5),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class EdgeEncodeBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.layers=torch.nn.Sequential(\n",
    "            ConvBNReLU(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBNReLU(out_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBNReLU(out_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class EdgeGuidanceBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv1=torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.conv2=torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, X, E):\n",
    "        gamma=torch.sigmoid(self.conv1(E))\n",
    "        beta=gamma*X\n",
    "        output=F.relu(self.conv2(gamma+beta))\n",
    "        return output\n",
    "\n",
    "class DeconvBNReLU(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=2, stride=2):\n",
    "        super().__init__()\n",
    "        self.layers=torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class DecodeBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, interpolate=True):\n",
    "        super().__init__()\n",
    "        if interpolate:\n",
    "            self.upsample=torch.nn.Sequential(\n",
    "                torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), \n",
    "                torch.nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n",
    "            )\n",
    "        else:\n",
    "            self.upsample=DeconvBNReLU(in_channels, out_channels)\n",
    "        self.conv=torch.nn.Sequential(\n",
    "            ConvBNReLU(out_channels*2, out_channels), \n",
    "            ConvBNReLU(out_channels, out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, X, E):\n",
    "        print('concatenate size x e', X.size(), E.size())\n",
    "        concatenated=torch.cat([E, self.upsample(X)], dim=1)\n",
    "        output=self.conv(concatenated)\n",
    "        return output\n",
    "\n",
    "class EdgeUNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=2, interpolate=True):\n",
    "        super().__init__()\n",
    "        self.image_encode_block1=ImageEncodeBlock1(in_channels, 64)\n",
    "        self.image_encode_block2=ImageEncodeBlock1(64, 128)\n",
    "        self.image_encode_block3=ImageEncodeBlock2(128, 256)\n",
    "        self.image_encode_block4=ImageEncodeBlock2(256, 512)\n",
    "        self.image_encode_block5=ImageEncodeBlock3(512, 1024)\n",
    "        self.maxpool=torch.nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.edge_encode_block1=EdgeEncodeBlock(in_channels, 64)\n",
    "        self.edge_encode_block2=EdgeEncodeBlock(64, 128)\n",
    "        self.edge_encode_block3=EdgeEncodeBlock(128, 256)\n",
    "        self.edge_encode_block4=EdgeEncodeBlock(256, 512)\n",
    "        self.avgpool=torch.nn.AvgPool2d(2, 2)\n",
    "\n",
    "        self.edge_guidance_block1=EdgeGuidanceBlock(64, 64)\n",
    "        self.edge_guidance_block2=EdgeGuidanceBlock(128, 128)\n",
    "        self.edge_guidance_block3=EdgeGuidanceBlock(256, 256)\n",
    "        self.edge_guidance_block4=EdgeGuidanceBlock(512, 512)\n",
    "\n",
    "        self.decode_block1=DecodeBlock(1024, 512, interpolate=interpolate)\n",
    "        self.decode_block2=DecodeBlock(512, 256, interpolate=interpolate)\n",
    "        self.decode_block3=DecodeBlock(256, 128, interpolate=interpolate)\n",
    "        self.decode_block4=DecodeBlock(128, 64, interpolate=interpolate)\n",
    "\n",
    "        self.out_conv=torch.nn.Conv2d(in_channels=64, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, image, edge):\n",
    "        image1=self.image_encode_block1(image)\n",
    "        downimg1=self.maxpool(image1)\n",
    "        image2=self.image_encode_block2(downimg1)\n",
    "        downimg2=self.maxpool(image2)\n",
    "        image3=self.image_encode_block3(downimg2)\n",
    "        downimg3=self.maxpool(image3)\n",
    "        image4=self.image_encode_block4(downimg3)\n",
    "        downimg4=self.maxpool(image4)\n",
    "        image5=self.image_encode_block5(downimg4)\n",
    "\n",
    "        edge1=self.edge_encode_block1(edge)\n",
    "        downedg1=self.avgpool(edge1)\n",
    "        edge2=self.edge_encode_block2(downedg1)\n",
    "        downedg2=self.avgpool(edge2)\n",
    "        edge3=self.edge_encode_block3(downedg2)\n",
    "        downedg3=self.avgpool(edge3)\n",
    "        edge4=self.edge_encode_block4(downedg3)\n",
    "\n",
    "        EGB1=self.edge_guidance_block1(image1, edge1)\n",
    "        EGB2=self.edge_guidance_block2(image2, edge2)\n",
    "        EGB3=self.edge_guidance_block3(image3, edge3)\n",
    "        EGB4=self.edge_guidance_block4(image4, edge4)\n",
    "\n",
    "        decode1=self.decode_block1(image5, EGB4)\n",
    "        decode2=self.decode_block2(decode1, EGB3)\n",
    "        decode3=self.decode_block3(decode2, EGB2)\n",
    "        decode4=self.decode_block4(decode3, EGB1)\n",
    "        \n",
    "        output=self.out_conv(decode4)\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unet network definition\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return torch.sigmoid(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process definition of unet\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model: UNet, \n",
    "          device: torch.device, \n",
    "          batch_size: int, \n",
    "          train_loader: DataLoader, \n",
    "          optimizer: torch.optim.Optimizer, \n",
    "          criterion: torch.nn.Module, \n",
    "          epoch: int):\n",
    "    '''\n",
    "    one epoch training process, containing: forwarding, calculating loss value, back propagation, printing some of the training progress\n",
    "    '''\n",
    "    running_loss=0.\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        inputs, targets, labels, edge=data\n",
    "        inputs, targets, labels=inputs.to(device), targets.to(device), labels.to(device)\n",
    "        optimizer.zero_grad() # set gradient of last epoch to zero\n",
    "        outputs=model(inputs)\n",
    "        loss=criterion(outputs, targets)\n",
    "        loss.backward() # backward the gradient\n",
    "        optimizer.step() # update parameters\n",
    "        running_loss+=loss.item() # sum of total loss\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('第{}轮次已训练{}批样本, 本批次平均loss值: {}'.format(epoch+1, batch_idx+1, running_loss/300))\n",
    "            running_loss=0.\n",
    "    return\n",
    "\n",
    "def eval(model: UNet, \n",
    "         device: torch.device, \n",
    "         test_loader: DataLoader,\n",
    "         criterion: torch.nn.Module):\n",
    "    '''\n",
    "    testing the accuracy of current partly-trained model and print\n",
    "    '''\n",
    "    correct=0\n",
    "    total=0\n",
    "    total_loss=0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, targets, labels, edge=data\n",
    "            images, targets, labels=images.to(device), targets.to(device), labels.to(device)\n",
    "            # outputs1, outputs2=model(images)\n",
    "            outputs2=model(images)\n",
    "            # _, predicted=torch.max(outputs1.data, dim=1)\n",
    "            total+=labels.size(0)\n",
    "            # correct+=(predicted-labels<0.5).sum().item()\n",
    "            total_loss+=criterion(outputs2, targets)\n",
    "    # print('accuracy on test set: {}%\\naverage dice score: {}'.format(100*correct/total, total_dice/total))\n",
    "    print(f'current epoch average cross entropy loss: {total_loss/total}')\n",
    "    image=np.array(images.cpu())\n",
    "    target=np.array(targets.cpu())\n",
    "    output2=np.array(outputs2.cpu())\n",
    "    output2[output2<=0.5]=0\n",
    "    output2[output2>0.5]=1\n",
    "    brain_MRI=image[0, 0].copy()\n",
    "    groun_truth=target[0].copy()\n",
    "    predict=output2[0, 1].copy()\n",
    "    print('Dice score', my_dice_score(predict, target))\n",
    "    plt.figure(figsize=(40, 40))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(brain_MRI, 'gray')\n",
    "    plt.title('original')\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(groun_truth, 'gray')\n",
    "    plt.title('target')\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(predict, 'gray')\n",
    "    plt.title('predict')\n",
    "    plt.show()\n",
    "    return total_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unet training process\n",
    "batch_size=1\n",
    "epochs=60\n",
    "model=UNet(1, 2, False)\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "model.to(device)\n",
    "transform=transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset=TumorDataset(dataset_dir='./dataset/training/', train=True, transform=transform)\n",
    "train_loader=DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_dataset=TumorDataset(dataset_dir='./dataset/testing/', train=False, transform=transform)\n",
    "test_loader=DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor([0.1, 0.9], device=device))\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), weight_decay=1e-8)\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(model, device, batch_size, train_loader, optimizer, criterion, epoch)\n",
    "    eval_loss=eval(model, device, test_loader, criterion)\n",
    "    scheduler.step(eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process definition of edge unet\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model: EdgeUNet, \n",
    "          device: torch.device, \n",
    "          batch_size: int, \n",
    "          train_loader: DataLoader, \n",
    "          optimizer: torch.optim.Optimizer, \n",
    "          criterion: torch.nn.Module, \n",
    "          epoch: int):\n",
    "    '''\n",
    "    one epoch training process, containing: forwarding, calculating loss value, back propagation, printing some of the training progress\n",
    "    '''\n",
    "    running_loss=0.\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        images, targets, labels, edges=data\n",
    "        edges=edges.reshape((-1, 1, 512, 512))\n",
    "        images, targets, labels, edges=images.to(device), targets.to(device), labels.to(device), edges.to(device)\n",
    "        optimizer.zero_grad() # set gradient of last epoch to zero\n",
    "        outputs=model(images, edges)\n",
    "        loss=criterion(outputs, targets)\n",
    "        loss.backward() # backward the gradient\n",
    "        optimizer.step() # update parameters\n",
    "        running_loss+=loss.item() # sum of total loss\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('第{}轮次已训练{}批样本, 本批次平均loss值: {}'.format(epoch+1, batch_idx+1, running_loss/300))\n",
    "            running_loss=0.\n",
    "    return\n",
    "\n",
    "def eval(model: EdgeUNet, \n",
    "         device: torch.device, \n",
    "         test_loader: DataLoader,\n",
    "         criterion: torch.nn.Module):\n",
    "    '''\n",
    "    testing the accuracy of current partly-trained model and print\n",
    "    '''\n",
    "    correct=0\n",
    "    total=0\n",
    "    total_loss=0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, targets, labels, edges=data\n",
    "            edges=edges.reshape((-1, 1, 512, 512))\n",
    "            images, targets, labels, edges=images.to(device), targets.to(device), labels.to(device), edges.to(device)\n",
    "            # outputs1, outputs2=model(images)\n",
    "            outputs2=model(images, edges)\n",
    "            # _, predicted=torch.max(outputs1.data, dim=1)\n",
    "            total+=labels.size(0)\n",
    "            # correct+=(predicted-labels<0.5).sum().item()\n",
    "            total_loss+=criterion(outputs2, targets)\n",
    "    # print('accuracy on test set: {}%\\naverage dice score: {}'.format(100*correct/total, total_dice/total))\n",
    "    print(f'current epoch average cross entropy loss: {total_loss/total}')\n",
    "    image=np.array(images.cpu())\n",
    "    target=np.array(targets.cpu())\n",
    "    output2=np.array(outputs2.cpu())\n",
    "    output2[output2<=0.5]=0\n",
    "    output2[output2>0.5]=1\n",
    "    brain_MRI=image[0, 0].copy()\n",
    "    groun_truth=target[0].copy()\n",
    "    predict=output2[0, 1].copy()\n",
    "    print('Dice score', my_dice_score(predict, target))\n",
    "    plt.figure(figsize=(40, 40))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(brain_MRI, 'gray')\n",
    "    plt.title('original')\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(groun_truth, 'gray')\n",
    "    plt.title('target')\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(predict, 'gray')\n",
    "    plt.title('predict')\n",
    "    plt.show()\n",
    "    return total_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "input size torch.Size([1, 1, 512, 512]) torch.Size([1, 1, 512, 512])\n",
      "image1 size torch.Size([1, 64, 512, 512])\n",
      "image2 size torch.Size([1, 128, 256, 256])\n",
      "image3 size torch.Size([1, 256, 128, 128])\n",
      "image4 size torch.Size([1, 512, 64, 64])\n",
      "image5 size torch.Size([1, 1024, 32, 32])\n",
      "edge1 size torch.Size([1, 64, 512, 512])\n",
      "edge2 size torch.Size([1, 128, 256, 256])\n",
      "edge3 size torch.Size([1, 256, 128, 128])\n",
      "edge4 size torch.Size([1, 512, 64, 64])\n",
      "egb1 size torch.Size([1, 64, 512, 512])\n",
      "egb2 size torch.Size([1, 128, 256, 256])\n",
      "egb3 size torch.Size([1, 256, 128, 128])\n",
      "egb4 size torch.Size([1, 512, 64, 64])\n",
      "concatenate size x e torch.Size([1, 1024, 32, 32]) torch.Size([1, 512, 64, 64])\n",
      "decode1 size torch.Size([1, 512, 64, 64])\n",
      "concatenate size x e torch.Size([1, 512, 64, 64]) torch.Size([1, 256, 128, 128])\n",
      "decode2 size torch.Size([1, 256, 128, 128])\n",
      "concatenate size x e torch.Size([1, 256, 128, 128]) torch.Size([1, 128, 256, 256])\n",
      "decode3 size torch.Size([1, 128, 256, 256])\n",
      "concatenate size x e torch.Size([1, 128, 256, 256]) torch.Size([1, 64, 512, 512])\n",
      "decode4 size torch.Size([1, 64, 512, 512])\n",
      "output size torch.Size([1, 2, 512, 512])\n",
      "input size torch.Size([1, 1, 512, 512]) torch.Size([1, 1, 512, 512])\n",
      "image1 size torch.Size([1, 64, 512, 512])\n",
      "image2 size torch.Size([1, 128, 256, 256])\n",
      "image3 size torch.Size([1, 256, 128, 128])\n",
      "image4 size torch.Size([1, 512, 64, 64])\n",
      "image5 size torch.Size([1, 1024, 32, 32])\n",
      "edge1 size torch.Size([1, 64, 512, 512])\n",
      "edge2 size torch.Size([1, 128, 256, 256])\n",
      "edge3 size torch.Size([1, 256, 128, 128])\n",
      "edge4 size torch.Size([1, 512, 64, 64])\n",
      "egb1 size torch.Size([1, 64, 512, 512])\n",
      "egb2 size torch.Size([1, 128, 256, 256])\n",
      "egb3 size torch.Size([1, 256, 128, 128])\n",
      "egb4 size torch.Size([1, 512, 64, 64])\n",
      "concatenate size x e torch.Size([1, 1024, 32, 32]) torch.Size([1, 512, 64, 64])\n",
      "decode1 size torch.Size([1, 512, 64, 64])\n",
      "concatenate size x e torch.Size([1, 512, 64, 64]) torch.Size([1, 256, 128, 128])\n",
      "decode2 size torch.Size([1, 256, 128, 128])\n",
      "concatenate size x e torch.Size([1, 256, 128, 128]) torch.Size([1, 128, 256, 256])\n",
      "decode3 size torch.Size([1, 128, 256, 256])\n",
      "concatenate size x e torch.Size([1, 128, 256, 256]) torch.Size([1, 64, 512, 512])\n",
      "decode4 size torch.Size([1, 64, 512, 512])\n",
      "output size torch.Size([1, 2, 512, 512])\n",
      "input size torch.Size([1, 1, 512, 512]) torch.Size([1, 1, 512, 512])\n",
      "image1 size torch.Size([1, 64, 512, 512])\n",
      "image2 size torch.Size([1, 128, 256, 256])\n",
      "image3 size torch.Size([1, 256, 128, 128])\n",
      "image4 size torch.Size([1, 512, 64, 64])\n",
      "image5 size torch.Size([1, 1024, 32, 32])\n",
      "edge1 size torch.Size([1, 64, 512, 512])\n",
      "edge2 size torch.Size([1, 128, 256, 256])\n",
      "edge3 size torch.Size([1, 256, 128, 128])\n",
      "edge4 size torch.Size([1, 512, 64, 64])\n",
      "egb1 size torch.Size([1, 64, 512, 512])\n",
      "egb2 size torch.Size([1, 128, 256, 256])\n",
      "egb3 size torch.Size([1, 256, 128, 128])\n",
      "egb4 size torch.Size([1, 512, 64, 64])\n",
      "concatenate size x e torch.Size([1, 1024, 32, 32]) torch.Size([1, 512, 64, 64])\n",
      "decode1 size torch.Size([1, 512, 64, 64])\n",
      "concatenate size x e torch.Size([1, 512, 64, 64]) torch.Size([1, 256, 128, 128])\n",
      "decode2 size torch.Size([1, 256, 128, 128])\n",
      "concatenate size x e torch.Size([1, 256, 128, 128]) torch.Size([1, 128, 256, 256])\n",
      "decode3 size torch.Size([1, 128, 256, 256])\n",
      "concatenate size x e torch.Size([1, 128, 256, 256]) torch.Size([1, 64, 512, 512])\n",
      "decode4 size torch.Size([1, 64, 512, 512])\n",
      "output size torch.Size([1, 2, 512, 512])\n",
      "input size torch.Size([1, 1, 512, 512]) torch.Size([1, 1, 512, 512])\n",
      "image1 size torch.Size([1, 64, 512, 512])\n",
      "image2 size torch.Size([1, 128, 256, 256])\n",
      "image3 size torch.Size([1, 256, 128, 128])\n",
      "image4 size torch.Size([1, 512, 64, 64])\n",
      "image5 size torch.Size([1, 1024, 32, 32])\n",
      "edge1 size torch.Size([1, 64, 512, 512])\n",
      "edge2 size torch.Size([1, 128, 256, 256])\n",
      "edge3 size torch.Size([1, 256, 128, 128])\n",
      "edge4 size torch.Size([1, 512, 64, 64])\n",
      "egb1 size torch.Size([1, 64, 512, 512])\n",
      "egb2 size torch.Size([1, 128, 256, 256])\n",
      "egb3 size torch.Size([1, 256, 128, 128])\n",
      "egb4 size torch.Size([1, 512, 64, 64])\n",
      "concatenate size x e torch.Size([1, 1024, 32, 32]) torch.Size([1, 512, 64, 64])\n",
      "decode1 size torch.Size([1, 512, 64, 64])\n",
      "concatenate size x e torch.Size([1, 512, 64, 64]) torch.Size([1, 256, 128, 128])\n",
      "decode2 size torch.Size([1, 256, 128, 128])\n",
      "concatenate size x e torch.Size([1, 256, 128, 128]) torch.Size([1, 128, 256, 256])\n",
      "decode3 size torch.Size([1, 128, 256, 256])\n",
      "concatenate size x e torch.Size([1, 128, 256, 256]) torch.Size([1, 64, 512, 512])\n",
      "decode4 size torch.Size([1, 64, 512, 512])\n",
      "output size torch.Size([1, 2, 512, 512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/dengjiaming/Code/Joint-Segmentation-and-Classification-for-Brain-Tumor-MRIs/EdgeUnet.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dengjiaming/Code/Joint-Segmentation-and-Classification-for-Brain-Tumor-MRIs/EdgeUnet.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m scheduler\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau(optimizer, patience\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dengjiaming/Code/Joint-Segmentation-and-Classification-for-Brain-Tumor-MRIs/EdgeUnet.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dengjiaming/Code/Joint-Segmentation-and-Classification-for-Brain-Tumor-MRIs/EdgeUnet.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     train(model, device, batch_size, train_loader, optimizer, criterion, epoch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dengjiaming/Code/Joint-Segmentation-and-Classification-for-Brain-Tumor-MRIs/EdgeUnet.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     eval_loss\u001b[39m=\u001b[39m\u001b[39meval\u001b[39m(model, device, test_loader, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dengjiaming/Code/Joint-Segmentation-and-Classification-for-Brain-Tumor-MRIs/EdgeUnet.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(eval_loss)\n",
      "\u001b[1;32m/Users/dengjiaming/Code/Joint-Segmentation-and-Classification-for-Brain-Tumor-MRIs/EdgeUnet.ipynb Cell 8\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, batch_size, train_loader, optimizer, criterion, epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dengjiaming/Code/Joint-Segmentation-and-Classification-for-Brain-Tumor-MRIs/EdgeUnet.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m outputs\u001b[39m=\u001b[39mmodel(images, edges)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dengjiaming/Code/Joint-Segmentation-and-Classification-for-Brain-Tumor-MRIs/EdgeUnet.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss\u001b[39m=\u001b[39mcriterion(outputs, targets)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dengjiaming/Code/Joint-Segmentation-and-Classification-for-Brain-Tumor-MRIs/EdgeUnet.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward() \u001b[39m# backward the gradient\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dengjiaming/Code/Joint-Segmentation-and-Classification-for-Brain-Tumor-MRIs/EdgeUnet.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep() \u001b[39m# update parameters\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dengjiaming/Code/Joint-Segmentation-and-Classification-for-Brain-Tumor-MRIs/EdgeUnet.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m running_loss\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39mitem() \u001b[39m# sum of total loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Edge unet training process\n",
    "batch_size=1\n",
    "epochs=3\n",
    "model=EdgeUNet(1, 2, False)\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())\n",
    "model.to(device)\n",
    "transform=transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset=TumorDataset(dataset_dir='./dataset/training/', train=True, transform=transform)\n",
    "train_loader=DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_dataset=TumorDataset(dataset_dir='./dataset/testing/', train=False, transform=transform)\n",
    "test_loader=DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "criterion=torch.nn.CrossEntropyLoss(weight=torch.tensor([0.1, 0.9], device=device))\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), weight_decay=1e-8)\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(model, device, batch_size, train_loader, optimizer, criterion, epoch)\n",
    "    eval_loss=eval(model, device, test_loader, criterion)\n",
    "    scheduler.step(eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54079b8dcdceced665d56fd9537317c787bb5c0871ed596a321875a01ff1e4b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
